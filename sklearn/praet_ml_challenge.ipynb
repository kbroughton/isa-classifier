{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praetorian ML Challenge - explore data\n",
    "\n",
    "Notes:\n",
    "On https://p16.praetorian.com/blog/machine-learning-tutorial the link \"Machine Learning Binaries\" is broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "This is a supervised learning task since we can guess wrong and get the correct answer for a given challenge; we can build a supervised training set\n",
    "\n",
    "Inspecting a few ISA's for the supported architectures, it appears there are typically hundreds of instructions, not tens of thousands, so a non-sparse matrix approach might work fine for a first principles text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting etl.py\n"
     ]
    }
   ],
   "source": [
    "# %load etl.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import base64\n",
    "import binascii\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "SUPPORTED_ARCHITECTURES = [\"avr\", \"alphaev56\", \"arm\", \"m68k\", \"mips\", \n",
    "                           \"mipsel\", \"powerpc\", \"s390\", \"sh4\", \"sparc\", \"x86_64\", \"xtensa\"]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "\n",
    "class Server(object):\n",
    "    url = 'https://mlb.praetorian.com'\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.session()\n",
    "        # in a sample of a few dozen, self.binary is either 32 or 36 bytes so pad X to 36 byte columns\n",
    "        self.binary  = None\n",
    "        self.hashes    = []\n",
    "        self.wins    = 0\n",
    "        self.targets = []\n",
    "        self.rate_limit_count = 0\n",
    "        self.unknown_server_exception_count = 0\n",
    "        self.retry_wait = 10\n",
    "        self.count = 0\n",
    "        self.failure_record = []\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.response = None\n",
    "        self.email = None\n",
    "\n",
    "    def _request(self, route, method='get', data=None):\n",
    "        while True:\n",
    "            if self.count > 597:\n",
    "                print('Resetting session at count {}'.format(self.count))\n",
    "                self.session = requests.session()\n",
    "            try:\n",
    "                if method == 'get':\n",
    "                    r = self.session.get(self.url + route)\n",
    "                else:\n",
    "                    r = self.session.post(self.url + route, data=data)\n",
    "                self.status_code = r.status_code\n",
    "                if r.status_code == 429:\n",
    "                    self.rate_limit_count += 1\n",
    "                    self.failure_record.append({'type': 'rate_limit',\n",
    "                                                'count': self.count,\n",
    "                                                'time': (datetime.datetime.now() -\n",
    "                                                         self.start_time).total_seconds()})\n",
    "                    raise Exception('Rate Limit Exception')\n",
    "                if r.status_code == 500:\n",
    "                    self.unknown_server_exception_count += 1\n",
    "                    self.failure_record.append({'type': 'unknown_server_exception',\n",
    "                                                'count': self.count,\n",
    "                                                'time': (datetime.datetime.now() -\n",
    "                                                         self.start_time).total_seconds()})\n",
    "                    raise Exception('Unknown Server Exception')\n",
    "                self.response = r\n",
    "                return r.json()\n",
    "            except Exception as e:\n",
    "                self.log.error(e)\n",
    "                self.log.info('Waiting 60 seconds before next request')\n",
    "                time.sleep(60)\n",
    "                self.status_code = None\n",
    "\n",
    "    def get(self):\n",
    "        r = self._request(\"/challenge\")\n",
    "        self.targets = r.get('target', [])\n",
    "        # removed base64.base64decode(r.get('binary', '')) to keep raw data raw\n",
    "        self.binary  = r.get('binary', '')\n",
    "        return r\n",
    "\n",
    "    def post(self, target):\n",
    "        r = self._request(\"/solve\", method=\"post\", data={\"target\": target})\n",
    "        self.wins = r.get('correct', 0)\n",
    "        hash = r.get('hash', None)\n",
    "        if hash:\n",
    "            self.log.info(\"You win! {}\".format(hash))\n",
    "            self.collect_hash()\n",
    "        self.ans  = r.get('target', 'unknown')\n",
    "        return r\n",
    "    \n",
    "    def collect_hash(self):\n",
    "        r = self._request(\"/hash\", method=\"post\", data={\"email\": self.email})\n",
    "        hash = r.get('hash', None)\n",
    "        print('hash_response is {}'.format(r))\n",
    "        self.hashes.append(hash)\n",
    "        # reset the session to earn more hashes\n",
    "        self.session.close()\n",
    "        self.session = requests.session()\n",
    "        \n",
    "    def get_data(self, number=10000, model=None, email=None):\n",
    "        \"\"\"\n",
    "        Retrieves data in format\n",
    "        @param number: nubmer of samples to return\n",
    "        @returns {binary_data: values, targets: values, answers: values}\n",
    "        where binary_data = [ \"<base64 encoded string>\", ... ]\n",
    "        targets =  [ \"avr\", \"x86_64\", ... ]\n",
    "        answers = [one item from targets,]\n",
    "        \"\"\"\n",
    "        if email:\n",
    "            self.email = email\n",
    "            \n",
    "        data = {}\n",
    "        # If we grab a large data set make it a little more efficient by pre-allocating\n",
    "        data['binary_data'] = [None]*number\n",
    "        data['targets'] = [None]*number\n",
    "        data['answers'] = [None]*number\n",
    "        for i in range(number):\n",
    "            self.count = 0\n",
    "            self.get()\n",
    "            while((self.status_code != 200) and (self.count < MAX_RETRIES) ):\n",
    "                print('Status code != 200. Retrying')\n",
    "                self.count = self.count + 1\n",
    "                self.get()\n",
    "            data['binary_data'][i] = self.binary\n",
    "            data['targets'][i] = self.targets\n",
    "            if model:\n",
    "                X = hex_data([self.binary])\n",
    "                probs = model.predict_proba(X)    # array of shape [1,12]\n",
    "                targets = class_to_ones_hot_targets([self.targets], model.classes_.tolist())\n",
    "                #print('data', X)\n",
    "                #print('probs', probs)\n",
    "                #print('targets', targets)\n",
    "                guess_arch = guess_arch_name(model, X, targets)\n",
    "            else:\n",
    "                guess_arch = self.targets[0]\n",
    "            self.post(guess_arch)\n",
    "            while((self.status_code != 200) and (self.count < MAX_RETRIES) ):\n",
    "                print('post status code {}'.format(self.status_code))\n",
    "                self.count = self.count + 1\n",
    "                self.post(guess_arch)\n",
    "            #print(i, type(i), self.ans, X, type(data['answers']))\n",
    "            data['answers'][i] = self.ans\n",
    "            \n",
    "        return data\n",
    "    \n",
    "    def get_data_sets(self, num_train=1024, num_test=128, num_dev=128, model=None, email=None):\n",
    "        \"\"\"\n",
    "        @param model: If we have a trained model, make trained guesses while downloading data\n",
    "        \n",
    "        @returns {'train': {'binary_data': [num_train values],\n",
    "                            'targets': [num_train [6 values]],\n",
    "                            'answers': [num_train values],\n",
    "                  'dev': format as with 'train',\n",
    "                  'test': format as with 'train'\n",
    "        }\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        data['train'] = self.get_data(num_train, model=model, email=email)\n",
    "        data['dev'] = self.get_data(num_dev, model=model, email=email)\n",
    "        data['test'] = self.get_data(num_test, model=model, email=email)\n",
    "        return data\n",
    "        \n",
    "\n",
    "# We only use the server class for interacting with the api\n",
    "# The methods below can be used with stored data    \n",
    "\n",
    "        \n",
    "def store(data, root_dir=None):\n",
    "    \"\"\"\n",
    "    Store data to root_dir in a generated filename\n",
    "    \"\"\"\n",
    "    # extract some info from data to generate friendly part of filename\n",
    "    num_train = str(len(data['train']['answers']))\n",
    "    num_dev = str(len(data['dev']['answers']))\n",
    "    num_test = str(len(data['test']['answers']))\n",
    "    \n",
    "    if not root_dir:\n",
    "        root_dir = os.getcwd()\n",
    "    else:\n",
    "        root_dir = os.path.realpath(root_dir)\n",
    "    file_name = \"_\".join([num_train, num_dev, num_test, str(uuid.uuid1())[0:8]]) + \".json\"\n",
    "    file_path = os.path.join(root_dir, file_name)\n",
    "    \n",
    "    with open(file_path, 'w+') as f:\n",
    "        json.dump(data, f)\n",
    "            \n",
    "def load(path):\n",
    "    \"\"\"\n",
    "    load data from a single file.  Data must have format specified in get_data_sets        \n",
    "    \"\"\"\n",
    "    path = os.path.realpath(path)\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_dir(path):\n",
    "    \"\"\"\n",
    "    load data from an entire directory.  Data must have format specified in get_data_sets\n",
    "    \"\"\"\n",
    "    merged = {'train': { 'answers': [],\n",
    "                         'targets': [],\n",
    "                         'binary_data': [] },\n",
    "              'dev': { 'answers': [],\n",
    "                         'targets': [],\n",
    "                         'binary_data': [] },\n",
    "              'test': { 'answers': [],\n",
    "                         'targets': [],\n",
    "                         'binary_data': [] }\n",
    "             }\n",
    "    if os.path.isdir(path):\n",
    "        for root, dirs, filenames in os.walk(path, topdown=True):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.json'):\n",
    "                    merged = merge_data(merged, load(os.path.join(root, filename)))\n",
    "    else:\n",
    "        merged = load(os.path.abspath(path)\n",
    ")\n",
    "    return merged\n",
    "\n",
    "def merge_data(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Perform list additions for two identically formatted dicts with lists at depth 2\n",
    "    \"\"\"\n",
    "    #_dict1 = deepcopy(dict1)\n",
    "    for key in dict2:\n",
    "        assert(key in dict1)\n",
    "        for key2 in dict2[key]:\n",
    "            assert(key2 in dict2[key])\n",
    "            dict1[key][key2] = dict1[key][key2] + dict2[key][key2]\n",
    "    return dict1\n",
    "\n",
    "def hex_data(base64_binary_data, stride=1, expected_len=None):\n",
    "    \"\"\"\n",
    "    Take an list of base64 encoded data and convert to lists of hex characters\n",
    "    @params base64_binary_data: a base64 encoded string of (presumeably) 32 or 36 hex numbers\n",
    "                                shape = (m, 1)\n",
    "    @params stride: number of bytes to consider a word\n",
    "    @params expected_len: max length of hex array, used if we need to pad shorter arrays\n",
    "    @returns: np.array([map(hexlify,base64string),])\n",
    "              shape = (m, len(base64.b64decode(base64_binary_data))/stride)\n",
    "    \"\"\"\n",
    "    # Should check that we aren't discarding https://docs.python.org/2/library/base64.html\n",
    "    # Characters that are neither in the normal base-64 alphabet nor the\n",
    "    # alternative alphabet are discarded prior to the padding check.\n",
    "    # Only python2 version does checking and uses stride, expected_len as it was found\n",
    "    # that data was uniformly 0 misfits.\n",
    "    misfits = []\n",
    "    hex_X = []\n",
    "\n",
    "    for data in base64_binary_data:\n",
    "        data = base64.b64decode(data)\n",
    "\n",
    "        if sys.version_info > (3,0):\n",
    "            hex_X.append(binascii.hexlify(data).decode('utf-8'))\n",
    "        else:\n",
    "            byte_strings = []\n",
    "            for i in range(0, len(data) , stride):\n",
    "                byte_strings.append(data[i:i+stride])\n",
    "            # probably not needed for most text_embeddings, but since most seem 32 or 36 it might help\n",
    "            if expected_len:\n",
    "                delta = len(byte_strings) - expected_len\n",
    "                div = delta/stride\n",
    "                remainder = delta % stride\n",
    "                if remainder or (delta < 0):\n",
    "                    print('misfit data', byte_strings)\n",
    "                else:\n",
    "                    byte_strings.extend(['\\x00'*stride]*div)\n",
    "            hex_X.append([ binascii.hexlify(e) for e in byte_strings ])\n",
    "\n",
    "    return np.array(hex_X)\n",
    "\n",
    "def class_to_ones_hot_answers(answers, classes):\n",
    "    \"\"\"\n",
    "    Take an list of answers as strings and convert to array of ones hot\n",
    "    \n",
    "    @param answers: (m, 1) array of strings\n",
    "    @param classes: len(classes) array of strings\n",
    "    @returns: (m, len(classes)) array of 0s and 1s\n",
    "    \"\"\"\n",
    "    get_arch_index = np.vectorize(lambda x: classes.index(x))\n",
    "    answer_indices = get_arch_index(np.array(answers))\n",
    "    hots = np.zeros(shape=(len(answers),len(classes)))\n",
    "    hots[np.arange(len(answers)), answer_indices] = 1\n",
    "    return hots\n",
    "\n",
    "def class_to_ones_hot_targets(targets, classes):\n",
    "    \"\"\"\n",
    "    Take an list of targets as strings and convert to array of ones hot\n",
    "    \n",
    "    @param answers: (m, 1) array of strings\n",
    "    @param classes: len(classes) array of strings\n",
    "    @returns: (m, len(classes)) array of 0s and 1s\n",
    "    \"\"\"\n",
    "    get_arch_index = np.vectorize(lambda x: classes.index(x))\n",
    "    target_indices = get_arch_index(np.array(targets))\n",
    "    hots = np.zeros(shape=(len(targets),len(classes)))\n",
    "    # loop over the six columns of targets array\n",
    "    for idx in range(target_indices.shape[1]):\n",
    "        hots[np.arange(len(targets)), target_indices[:,idx]] = 1\n",
    "    return hots\n",
    "\n",
    "def class_to_ones_hot(answers, targets, classes):\n",
    "    \"\"\"\n",
    "    Converts lists to arrays and strings to ones-hot\n",
    "    \n",
    "    param answers: length m list of strings\n",
    "    param targets: length m list of [6 strings]\n",
    "    @returns (m, len(classes) answers, (m, len(classes) targets in ones-hot\n",
    "    \"\"\"\n",
    "    return class_to_ones_hot_answers(answers, classes), class_to_ones_hot_targets(targets, classes) \n",
    "\n",
    "\n",
    "def guess_arch_name(model, X, allowed_Y, use_targets=True):\n",
    "    \"\"\"\n",
    "    Get the arch name prediction from the probabilities in ones-hot\n",
    "    \n",
    "    model: model trained on (m, 1) input\n",
    "    X: numerical array of shape (m, 1)\n",
    "    targets: ones-hot array of shape (m, n_classes), ignored if use_targets = False\n",
    "    use_targets: if True, Improve our chances by taking the max over the possible targets (6 instead of 12)\n",
    "    \n",
    "    returns: (m, 1) of the most likely ISA arch names \n",
    "    \"\"\"\n",
    "    probs = model.predict_proba(X)\n",
    "    get_arch = np.vectorize(lambda x: model.classes_.__getitem__(x))\n",
    "    if use_targets:\n",
    "        return get_arch(np.argmax(probs*allowed_Y, axis=1))\n",
    "    else:\n",
    "        return get_arch(np.argmax(probs, axis=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/isa-classifier/sklearn\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from -r ../requirements.txt (line 1))\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->-r ../requirements.txt (line 1))\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->-r ../requirements.txt (line 1))\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->-r ../requirements.txt (line 1))\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->-r ../requirements.txt (line 1))\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data while guessing with the model trained below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# handy when developing etl in this notebook\n",
    "from importlib import reload\n",
    "if 'etl' in globals():\n",
    "    reload(etl)\n",
    "else:\n",
    "    import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s = etl.Server()\n",
    "data_set_new = s.get_data_sets(num_train=2048,\n",
    "                               num_test=256,\n",
    "                               num_dev=256,\n",
    "                               model=mnb_model,\n",
    "                               email='kesten.broughton@gmail.com')\n",
    "etl.store(data_set_new, '../ml_challenge/')\n",
    "\n",
    "# If this errors on mnb_model is not defined, you must run the training below first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch previously collected data on first run of this notebook\n",
    "Fetch some data in the format supplied by etl.py (train, dev, test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only run this cell the first time\n",
    "%cd ../\n",
    "!wget https://s3.us-east-2.amazonaws.com/isa-classifier/isa_classifier_data.tar.gz\n",
    "!tar xvf isa_classifier_data.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/notebooks/projects/isa-classifier/sklearn\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024_128_128_eba414fa.json  2048_256_256_9e7a4848.json\r\n",
      "2048_256_256_075b60e8.json  2048_256_256_b951fec6.json\r\n",
      "2048_256_256_0b966df8.json  2048_256_256_cb0707f0.json\r\n",
      "2048_256_256_0baba5f4.json  2048_256_256_dd02089e.json\r\n",
      "2048_256_256_2847388a.json  2048_256_256_dd67c64e.json\r\n",
      "2048_256_256_666e0b7e.json  4096_512_512_6f9ef7d8.json\r\n",
      "2048_256_256_8f6606b2.json  4096_512_512_94537036.json\r\n",
      "2048_256_256_98813c7c.json  512_128_128_76fe753c.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../ml_challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etl\n",
    "\n",
    "# For more accurate results you will need > 10Gb server and \n",
    "# data_set = etl.load_dir('../ml_challenge/')\n",
    "data_set = etl.load('../ml_challenge/2048_256_256_dd67c64e.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_set['train']['binary_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_X_train[0:4] \n",
      " ['AADtjAAywX8ADD0gAADACQAA7AsAMuwMACj9gAIQPSAAAMAJAAD/jAAAQJ0ADMAfAAhIAAAQwZ8ACMAfAAzsDA==', 'AAb/////AAAAAAADbGx4CgAgAJgLAKkRwCAAmBuQkPWgmSDAIACZCMAgAJgrwCAAmQjBAADRAAChAACBAADgCA==', 'jIkAGI1LAACNaQAAAShIJK1pAACMSQAgJSkAARAA//GsSQAgjUIAAK+iABiMwgAIAAAwIYxCAACsogAMPAIAAA==', 'WDAwAFAwEACnSAAAWDDR4lBAEABYQNHeUEAQAFhAMABQQBAAWEAwBFBAEABYQNHaWDAwCFAwEABYMEAAUDAQAA==']\n",
      "orig_Y_train[0:4] \n",
      " ['powerpc', 'xtensa', 'mips', 's390']\n",
      "orig_train_targets[0:4] \n",
      " [['alphaev56', 'arm', 'powerpc', 's390', 'sh4', 'xtensa'], ['alphaev56', 'avr', 'mipsel', 'powerpc', 'sh4', 'xtensa'], ['avr', 'm68k', 'mips', 'powerpc', 'sh4', 'sparc'], ['arm', 'avr', 'm68k', 'mips', 'mipsel', 's390']]\n"
     ]
    }
   ],
   "source": [
    "orig_X_train, orig_Y_train, orig_train_targets = data_set['train']['binary_data'], data_set['train']['answers'], data_set['train']['targets']\n",
    "orig_X_dev, orig_Y_dev, orig_dev_targets = data_set['dev']['binary_data'], data_set['dev']['answers'], data_set['dev']['targets']\n",
    "orig_X_test, orig_Y_test, orig_test_targets = data_set['test']['binary_data'], data_set['test']['answers'], data_set['test']['targets']\n",
    "\n",
    "print('orig_X_train[0:4]', '\\n', orig_X_train[0:4])\n",
    "print('orig_Y_train[0:4]', '\\n', orig_Y_train[0:4])\n",
    "print('orig_train_targets[0:4]', '\\n', orig_train_targets[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hex_X_train[0:4] [ '0000ed8c0032c17f000c3d200000c0090000ec0b0032ec0c0028fd8002103d200000c0090000ff8c0000409d000cc01f000848000010c19f0008c01f000cec0c'\n",
      " '0006ffffffff0000000000036c6c780a002000980b00a911c02000981b9090f5a09920c020009908c02000982bc020009908c10000d10000a10000810000e008'\n",
      " '8c8900188d4b00008d69000001284824ad6900008c490020252900011000fff1ac4900208d420000afa200188cc20008000030218c420000aca2000c3c020000'\n",
      " '5830300050301000a74800005830d1e2504010005840d1de50401000584030005040100058403004504010005840d1da58303008503010005830400050301000'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "hex_X_train = etl.hex_data(orig_X_train)\n",
    "hex_X_dev = etl.hex_data(orig_X_dev)\n",
    "hex_X_test = etl.hex_data(orig_X_test)\n",
    "\n",
    "print('hex_X_train[0:4]', hex_X_train[0:4], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "idf_opts = {\n",
    "    \"ngram_range\": (1, 6),  # allow n-grams of 1-6 words in length (32-bits)\n",
    "    \"analyzer\": \"word\",     # analyze hex words\n",
    "    \"token_pattern\": \"..\",  # treat two characters as a word (e.g. 4b)\n",
    "    \"min_df\": 2,          # for demo purposes, be very selective about features\n",
    "    \"max_df\": .7\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('idf',  TfidfVectorizer(**idf_opts)),\n",
    "    ('mnb_classifier', MultinomialNB(alpha=1e-4))\n",
    "])\n",
    "\n",
    "mnb_model = pipeline.fit(hex_X_train, orig_Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### express correct arch and targets in ones-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_train[0:4] \n",
      " [[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]] \n",
      "\n",
      "allowed_Y_train[0:4] \n",
      " [[ 1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.]\n",
      " [ 1.  0.  1.  0.  0.  1.  1.  0.  1.  0.  0.  1.]\n",
      " [ 0.  0.  1.  1.  1.  0.  1.  0.  1.  1.  0.  0.]\n",
      " [ 0.  1.  1.  1.  1.  1.  0.  1.  0.  0.  0.  0.]] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Y_train, allowed_Y_train = etl.class_to_ones_hot(orig_Y_train, orig_train_targets, mnb_model.classes_.tolist())\n",
    "Y_dev, allowed_Y_dev = etl.class_to_ones_hot(orig_Y_dev, orig_dev_targets, mnb_model.classes_.tolist())\n",
    "Y_test, allowed_Y_test = etl.class_to_ones_hot(orig_Y_test, orig_test_targets, mnb_model.classes_.tolist())\n",
    "\n",
    "print('Y_train[0:4]', '\\n', Y_train[0:4], '\\n')\n",
    "print('allowed_Y_train[0:4]', '\\n', allowed_Y_train[0:4], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idf': TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=0.7, max_features=None, min_df=2,\n",
       "         ngram_range=(1, 6), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "         stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "         token_pattern='..', tokenizer=None, use_idf=True, vocabulary=None),\n",
       " 'mnb_classifier': MultinomialNB(alpha=0.0001, class_prior=None, fit_prior=True)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ed', 52205), ('8c', 40562), ('32', 27544), ('c1', 47209), ('7f', 37009), ('0c', 16876), ('3d', 28921), ('20', 21609), ('c0', 46629), ('09', 16144)]\n",
      "55389\n"
     ]
    }
   ],
   "source": [
    "print(list(mnb_model.named_steps['idf'].vocabulary_.items())[0:10])\n",
    "print(len(mnb_model.named_steps['idf'].vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alphaev56', 'arm', 'avr', 'm68k', 'mips', 'mipsel', 'powerpc', 's390', 'sh4', 'sparc', 'x86_64', 'xtensa']\n"
     ]
    }
   ],
   "source": [
    "print(mnb_model.classes_.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw probs\n",
      "[[  3.47346486e-46   1.85174299e-44   3.84898312e-45   1.18398949e-44\n",
      "    8.79953096e-45   7.36921467e-43   1.00000000e+00   3.48942071e-44\n",
      "    3.42281558e-44   2.10079016e-44   2.90184092e-45   1.10689350e-42]\n",
      " [  1.22862989e-38   1.43118953e-36   5.02068075e-42   8.39340188e-35\n",
      "    1.11558564e-38   5.36644684e-41   6.53898977e-35   8.10018136e-38\n",
      "    5.58671351e-37   1.69716228e-40   3.25690908e-38   1.00000000e+00]]\n",
      "allowed probs\n",
      "[[  3.47346486e-46   1.85174299e-44   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   3.48942071e-44\n",
      "    3.42281558e-44   0.00000000e+00   0.00000000e+00   1.10689350e-42]\n",
      " [  1.22862989e-38   0.00000000e+00   5.02068075e-42   0.00000000e+00\n",
      "    0.00000000e+00   5.36644684e-41   6.53898977e-35   0.00000000e+00\n",
      "    5.58671351e-37   0.00000000e+00   0.00000000e+00   1.00000000e+00]]\n",
      "allowed\n",
      "[[ 1.  1.  0.  0.  0.  0.  1.  1.  1.  0.  0.  1.]\n",
      " [ 1.  0.  1.  0.  0.  1.  1.  0.  1.  0.  0.  1.]]\n",
      "[ 6 11]\n",
      "['powerpc', 'xtensa', 'mips', 's390', 's390', 'alphaev56', 'alphaev56', 'x86_64', 'powerpc', 'mips']\n",
      "correct\n",
      "['powerpc', 'xtensa', 'mips', 's390', 's390', 'alphaev56', 'alphaev56', 'x86_64', 'powerpc', 'mips']\n",
      "[[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "probs_train = mnb_model.predict_proba(hex_X_train)\n",
    "print(\"raw probs\")\n",
    "print(probs_train[0:2])\n",
    "print(\"allowed probs\")\n",
    "print(probs_train[0:2]*allowed_Y_train[0:2])\n",
    "print(\"allowed\")\n",
    "print(allowed_Y_train[0:2])\n",
    "print(np.argmax(probs_train[0:2]*allowed_Y_train[0:2], axis=1))\n",
    "print(list(map(mnb_model.classes_.tolist().__getitem__, np.argmax(probs_train[0:10]*allowed_Y_train[0:10], axis=1))))\n",
    "print(\"correct\")\n",
    "print(orig_Y_train[0:10])\n",
    "print(Y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_train = etl.guess_arch_name(mnb_model, hex_X_train, allowed_Y_train, use_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('powerpc', 'powerpc')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guess_train[0], orig_Y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the class distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>m68k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           0\n",
       "count   2048\n",
       "unique    12\n",
       "top     m68k\n",
       "freq     192"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D = pd.DataFrame(guess_train)\n",
    "D.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.Series(orig_Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "powerpc      192\n",
       "m68k         191\n",
       "x86_64       188\n",
       "xtensa       184\n",
       "sh4          182\n",
       "mips         177\n",
       "arm          168\n",
       "alphaev56    167\n",
       "sparc        151\n",
       "s390         150\n",
       "mipsel       150\n",
       "avr          148\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a pretty good distribution of classes < 10% variance in counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_results(predictions, orig_Y):\n",
    "    wrong = []\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != orig_Y[i]:\n",
    "            wrong.append([predictions[i], orig_Y[i]])\n",
    "    print('training error: {}%'.format(100.0*len(wrong)/len(predictions)))\n",
    "    print('some mistakes')\n",
    "    print(wrong[0:15])\n",
    "    return wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.048828125%\n",
      "some mistakes\n",
      "[['m68k', 'xtensa']]\n"
     ]
    }
   ],
   "source": [
    "wrong = describe_results(guess_train, orig_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>m68k</td>\n",
       "      <td>xtensa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       incorrect correct\n",
       "count          1       1\n",
       "unique         1       1\n",
       "top         m68k  xtensa\n",
       "freq           1       1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw = pd.DataFrame(wrong)\n",
    "pdw.columns = ['incorrect', 'correct']\n",
    "pdw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">xtensa</th>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>m68k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               incorrect\n",
       "correct                 \n",
       "xtensa  count          1\n",
       "        unique         1\n",
       "        top         m68k\n",
       "        freq           1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw.groupby(by=pdw.correct).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is worst at predicting powerpc, then sh4 and sparc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>incorrect</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">m68k</th>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>xtensa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 correct\n",
       "incorrect               \n",
       "m68k      count        1\n",
       "          unique       1\n",
       "          top     xtensa\n",
       "          freq         1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw.groupby(by=pdw.incorrect).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m68k</td>\n",
       "      <td>xtensa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  incorrect correct\n",
       "0      m68k  xtensa"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most false positives come from x86_64 and m68k, we are slightly over-fitting to them\n",
    "pdw[pdw.incorrect == 'm68k'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>m68k</td>\n",
       "      <td>xtensa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  incorrect correct\n",
       "0      m68k  xtensa"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [incorrect, correct]\n",
       "Index: []"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw[pdw.correct == 'sparc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repeat for dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256\n",
      "[  2.06932784e-13   3.90070797e-15   6.27651073e-17   3.38994007e-11\n",
      "   2.58364985e-13   1.22955623e-15   1.00000000e+00   1.05945217e-15\n",
      "   2.33171986e-12   2.71414387e-18   1.29291917e-13   1.32116605e-14] [ 1.  0.  0.  1.  0.  0.  1.  1.  1.  0.  1.  0.] ['alphaev56', 'm68k', 'powerpc', 's390', 'sh4', 'x86_64']\n"
     ]
    }
   ],
   "source": [
    "probs_dev = mnb_model.predict_proba(hex_X_dev)\n",
    "\n",
    "print(len(probs_dev), len(allowed_Y_dev))\n",
    "print(probs_dev[0], allowed_Y_dev[0], orig_dev_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dev = etl.guess_arch_name(mnb_model, hex_X_dev, allowed_Y_dev, use_targets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training error: 0.390625%\n",
      "some mistakes\n",
      "[['m68k', 'xtensa']]\n"
     ]
    }
   ],
   "source": [
    "wrong_dev = describe_results(predictions_dev, orig_Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99609375"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb_model.score(hex_X_dev, orig_Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>m68k</td>\n",
       "      <td>xtensa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       incorrect correct\n",
       "count          1       1\n",
       "unique         1       1\n",
       "top         m68k  xtensa\n",
       "freq           1       1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw = pd.DataFrame(wrong_dev)\n",
    "pdw.columns = ['incorrect', 'correct']\n",
    "pdw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>incorrect</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>correct</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">xtensa</th>\n",
       "      <th>count</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>m68k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               incorrect\n",
       "correct                 \n",
       "xtensa  count          1\n",
       "        unique         1\n",
       "        top         m68k\n",
       "        freq           1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdw.groupby(by=pdw.correct).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "With the complete training set ~ 24k samples - we see .01% error rate for train and .3% for dev.\n",
    "We are substantially over-fitting, but it is sufficient to easily get 500 correct guesses in a row.<br>\n",
    "\n",
    "*TODO*: reduce overfitting in the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
