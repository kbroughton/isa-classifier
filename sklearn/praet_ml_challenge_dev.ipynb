{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Praetorian ML Challenge - explore data\n",
    "\n",
    "### This is the same as praet_ml_challenge notebook with a little more rough work included\n",
    "\n",
    "Notes:\n",
    "On https://p16.praetorian.com/blog/machine-learning-tutorial the link \"Machine Learning Binaries\" is broken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "At first glance, this may seem like an unsupervised task, but since we can guess wrong and get the correct answer for a given challenge, we can build a supervised training set\n",
    "\n",
    "Inspecting a few ISA's for the supported architectures, it appears there are typically hundreds of instructions, not tens of thousands, so a non-sparse matrix approach might work fine for a first principles text embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load etl.py\n",
    "\n",
    "from __future__ import print_function\n",
    "import base64\n",
    "import binascii\n",
    "import datetime\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "SUPPORTED_ARCHITECTURES = [\"avr\", \"alphaev56\", \"arm\", \"m68k\", \"mips\", \n",
    "                           \"mipsel\", \"powerpc\", \"s390\", \"sh4\", \"sparc\", \"x86_64\", \"xtensa\"]\n",
    "\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "class Server(object):\n",
    "    url = 'https://mlb.praetorian.com'\n",
    "    log = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self):\n",
    "        self.session = requests.session()\n",
    "        # in a sample of a few dozen, self.binary is either 32 or 36 bytes so pad X to 36 byte columns\n",
    "        self.binary  = None\n",
    "        self.hash    = None\n",
    "        self.wins    = 0\n",
    "        self.targets = []\n",
    "        self.rate_limit_count = 0\n",
    "        self.unknown_server_exception_count = 0\n",
    "        self.retry_wait = 10\n",
    "        self.count = 0\n",
    "        self.failure_record = []\n",
    "        self.start_time = datetime.datetime.now()\n",
    "        self.response = None\n",
    "\n",
    "    def _request(self, route, method='get', data=None):\n",
    "        while True:\n",
    "            if self.count > 597:\n",
    "                print('Resetting session at count {}'.format(self.count))\n",
    "                self.session = requests.session()\n",
    "            try:\n",
    "                if method == 'get':\n",
    "                    r = self.session.get(self.url + route)\n",
    "                else:\n",
    "                    r = self.session.post(self.url + route, data=data)\n",
    "                self.status_code = r.status_code\n",
    "                if r.status_code == 429:\n",
    "                    self.rate_limit_count += 1\n",
    "                    self.failure_record.append({'type': 'rate_limit',\n",
    "                                                'count': self.count,\n",
    "                                                'time': (datetime.datetime.now() -\n",
    "                                                         self.start_time).total_seconds()})\n",
    "                    raise Exception('Rate Limit Exception')\n",
    "                if r.status_code == 500:\n",
    "                    self.unknown_server_exception_count += 1\n",
    "                    self.failure_record.append({'type': 'unknown_server_exception',\n",
    "                                                'count': self.count,\n",
    "                                                'time': (datetime.datetime.now() -\n",
    "                                                         self.start_time).total_seconds()})\n",
    "                    raise Exception('Unknown Server Exception')\n",
    "                self.response = r\n",
    "                return r.json()\n",
    "            except Exception as e:\n",
    "                self.log.error(e)\n",
    "                self.log.info('Waiting 60 seconds before next request')\n",
    "                time.sleep(60)\n",
    "                self.status_code = None\n",
    "\n",
    "    def get(self):\n",
    "        r = self._request(\"/challenge\")\n",
    "        self.targets = r.get('target', [])\n",
    "        # removed base64.base64decode(r.get('binary', '')) to allow writes to disk without re-encoding\n",
    "        self.binary  = r.get('binary', '')\n",
    "        return r\n",
    "\n",
    "    def post(self, target):\n",
    "        r = self._request(\"/solve\", method=\"post\", data={\"target\": target})\n",
    "        self.wins = r.get('correct', 0)\n",
    "        self.hash = r.get('hash', self.hash)\n",
    "        self.ans  = r.get('target', 'unknown')\n",
    "        return r\n",
    "    \n",
    "    def get_data(self, number=10000):\n",
    "        \"\"\"\n",
    "        Retrieves data in format\n",
    "        @param number: nubmer of samples to return\n",
    "        @returns {binary_data: values, targets: values, answers: values}\n",
    "        where binary_data = [ \"<base64 encoded string>\", ... ]\n",
    "        targets =  [ \"avr\", \"x86_64\", ... ]\n",
    "        answers = [one item from targets,]\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        # If we grab a large data set make it a little more efficient by pre-allocating\n",
    "        data['binary_data'] = [None]*number\n",
    "        data['targets'] = [None]*number\n",
    "        data['answers'] = [None]*number\n",
    "        for i in range(number):\n",
    "            count = 0\n",
    "            self.get()\n",
    "            while((self.status_code != 200) and (count < MAX_RETRIES) ):\n",
    "                print('Status code != 200. Retrying')\n",
    "                count = count + 1\n",
    "                self.get()\n",
    "            data['binary_data'][i] = self.binary\n",
    "            data['targets'][i] = self.targets\n",
    "            self.post(self.targets[0])\n",
    "            while((self.status_code != 200) and (count < MAX_RETRIES) ):\n",
    "                print('post status code {}'.format(self.status_code))\n",
    "                count = count + 1\n",
    "                self.post(self.targets[0])\n",
    "            data['answers'][i] = self.ans\n",
    "        return data\n",
    "    \n",
    "    def get_data_sets(self, num_train=1024, num_test=128, num_dev=128):\n",
    "        \"\"\"\n",
    "        @returns {'train': {'binary_data': [num_train values],\n",
    "                            'targets': [num_train [6 values]],\n",
    "                            'answers': [num_train values],\n",
    "                  'dev': format as with 'train',\n",
    "                  'test': format as with 'train'\n",
    "        }\n",
    "        @param name_prefix: Write data to a file of this name. If None, do not write\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        data['train'] = self.get_data(num_train)\n",
    "        data['dev'] = self.get_data(num_dev)\n",
    "        data['test'] = self.get_data(num_test)\n",
    "        return data\n",
    "        \n",
    "        \n",
    "def submit(number):\n",
    "    s = Server()\n",
    "\n",
    "    for _ in range(number):\n",
    "        # query the /challenge endpoint\n",
    "        s.get()\n",
    "\n",
    "        # choose a random target and /solve\n",
    "        target = random.choice(s.targets)\n",
    "        s.post(target)\n",
    "\n",
    "        s.log.info(\"Guess:[{: >9}]   Answer:[{: >9}]   Wins:[{: >3}]\".format(target, s.ans, s.wins))\n",
    "\n",
    "        # 500 consecutive correct answers are required to win\n",
    "        # very very unlikely with current code\n",
    "        if s.hash:\n",
    "            s.log.info(\"You win! {}\".format(s.hash))    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def store(data, root_dir=None):\n",
    "    \"\"\"\n",
    "    Store data to root_dir in a generated filename\n",
    "    \"\"\"\n",
    "    # extract some info from data to generate friendly part of filename\n",
    "    num_train = str(len(data['train']['answers']))\n",
    "    num_dev = str(len(data['dev']['answers']))\n",
    "    num_test = str(len(data['test']['answers']))\n",
    "    \n",
    "    if not root_dir:\n",
    "        root_dir = os.getcwd()\n",
    "    else:\n",
    "        root_dir = os.path.realpath(root_dir)\n",
    "    file_name = \"_\".join([num_train, num_dev, num_test, str(uuid.uuid1())[0:8]]) + \".json\"\n",
    "    file_path = os.path.join(root_dir, file_name)\n",
    "    \n",
    "    with open(file_path, 'w+') as f:\n",
    "        json.dump(data, f)\n",
    "            \n",
    "def load(path):\n",
    "    \"\"\"\n",
    "    load data from a single file.  Data must have format specified in get_data_sets        \n",
    "    \"\"\"\n",
    "    path = os.path.realpath(path)\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_dir(path):\n",
    "    \"\"\"\n",
    "    load data from an entire directory.  Data must have format specified in get_data_sets\n",
    "    \"\"\"\n",
    "    merged = {'train': { 'answers': [],\n",
    "                         'targets': [],\n",
    "                         'binary_data': [] },\n",
    "              'dev': { 'answers': [],\n",
    "                         'targets': [],\n",
    "                         'binary_data': [] },\n",
    "              'test': { 'answers': [],\n",
    "                         'targets': [],\n",
    "                         'binary_data': [] }\n",
    "             }\n",
    "    if os.path.isdir(path):\n",
    "        for root, dirs, filenames in os.walk(path, topdown=True):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith('.json'):\n",
    "                    merged = merge_data(merged, load(os.path.join(root, filename)))\n",
    "    else:\n",
    "        merged = load(os.path.abspath(path)\n",
    ")\n",
    "    return merged\n",
    "\n",
    "def merge_data(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Perform list additions for two identically formatted dicts with lists at depth 2\n",
    "    \"\"\"\n",
    "    #_dict1 = deepcopy(dict1)\n",
    "    for key in dict2:\n",
    "        assert(key in dict1)\n",
    "        for key2 in dict2[key]:\n",
    "            assert(key2 in dict2[key])\n",
    "            dict1[key][key2] = dict1[key][key2] + dict2[key][key2]\n",
    "    return dict1\n",
    "\n",
    "def hex_data(base64_binary_data, stride=1, expected_len=None):\n",
    "    \"\"\"\n",
    "    Take an list of base64 encoded data and convert to lists of hex characters\n",
    "    @params base64_binary_data: a base64 encoded string of (presumeably) 32 or 36 hex numbers\n",
    "                                shape = (m, 1)\n",
    "    @params stride: number of bytes to consider a word\n",
    "    @params expected_len: max length of hex array, used if we need to pad shorter arrays\n",
    "    @returns: np.array([map(hexlify,base64string),])\n",
    "              shape = (m, len(base64.b64decode(base64_binary_data))/stride)\n",
    "    \"\"\"\n",
    "    # Should check that we aren't discarding https://docs.python.org/2/library/base64.html\n",
    "    # Characters that are neither in the normal base-64 alphabet nor the\n",
    "    # alternative alphabet are discarded prior to the padding check.\n",
    "    # Only python2 version does checking and uses stride, expected_len as it was found\n",
    "    # that data was uniformly 0 misfits.\n",
    "    misfits = []\n",
    "    hex_X = []\n",
    "\n",
    "    for data in base64_binary_data:\n",
    "        data = base64.b64decode(data)\n",
    "\n",
    "        if sys.version_info > (3,0):\n",
    "            hex_X.append(binascii.hexlify(data).decode('utf-8'))\n",
    "        else:\n",
    "            byte_strings = []\n",
    "            for i in range(0, len(data) , stride):\n",
    "                byte_strings.append(data[i:i+stride])\n",
    "            # probably not needed for most text_embeddings, but since most seem 32 or 36 it might help\n",
    "            if expected_len:\n",
    "                delta = len(byte_strings) - expected_len\n",
    "                div = delta/stride\n",
    "                remainder = delta % stride\n",
    "                if remainder or (delta < 0):\n",
    "                    print('misfit data', byte_strings)\n",
    "                else:\n",
    "                    byte_strings.extend(['\\x00'*stride]*div)\n",
    "            hex_X.append([ binascii.hexlify(e) for e in byte_strings ])\n",
    "\n",
    "    return np.array(hex_X)\n",
    "\n",
    "def class_to_ones_hot(answers, targets, supported_architectures):\n",
    "    Y = []\n",
    "    allowed_Y = []\n",
    "    for answer, target in zip(answers, targets):\n",
    "        y = [0]*len(supported_architectures)\n",
    "        index = supported_architectures.index(answer)\n",
    "        y[index] = 1\n",
    "        Y.append(y)\n",
    "        target_hot = [0]*len(supported_architectures)\n",
    "        for j, arch in enumerate(target):\n",
    "            index = supported_architectures.index(arch)\n",
    "            target_hot[index] = 1\n",
    "        allowed_Y.append(target_hot)\n",
    "    return np.array(Y), np.array(allowed_Y) \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch data on first run of this notebook\n",
    "Fetch some data in the format supplied by etl.py (train, dev, test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only run this cell the first time\n",
    "%cd ../\n",
    "!wget https://s3.us-east-2.amazonaws.com/isa-classifier/isa_classifier_data.tar.gz\n",
    "!tar xvf isa_classifier_data.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../ml_challenge\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (skip?) Get and store new data\n",
    "We will store the data in as close to the original form as possible.\n",
    "We will do a small sample walkthrough of loading data and transforming it here as far as \n",
    "binary_data = [ [ 32 or 36 hex numbers ], ...].\n",
    "We also transform the ISA class into one-hot vectors.\n",
    "The word embedding transformations will be done in text_embeddings/I-TFID.ipynb, word2vec.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etl\n",
    "s = etl.Server()\n",
    "data_set1 = s.get_data_sets(num_train=4096, num_test=512, num_dev=512)\n",
    "etl.store(data_set1, '../ml_challenge/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('binary_data', data_set1['train']['binary_data'][0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../ml_challenge/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import etl\n",
    "\n",
    "data_set = etl.load('../ml_challenge/2048_256_256_dd67c64e.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_set['train']['binary_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_X_train, orig_Y_train, orig_train_targets = data_set['train']['binary_data'], data_set['train']['answers'], data_set['train']['targets']\n",
    "orig_X_dev, orig_Y_dev, orig_dev_targets = data_set['dev']['binary_data'], data_set['dev']['answers'], data_set['dev']['targets']\n",
    "orig_X_test, orig_Y_test, orig_test_targets = data_set['test']['binary_data'], data_set['test']['answers'], data_set['test']['targets']\n",
    "\n",
    "print('orig_X_train[0:4]', '\\n', orig_X_train[0:4])\n",
    "print('orig_Y_train[0:4]', '\\n', orig_Y_train[0:4])\n",
    "print('orig_train_targets[0:4]', '\\n', orig_train_targets[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "a = b'00011000'\n",
    "print(int(a))\n",
    "b = hex(int(a))\n",
    "print(b)\n",
    "binascii.hexlify(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "base64_encoded = u'GAAAABgAAAAAAAAAqAAAAABIDhBHmgJa2g4AAAAAQaAAAD0kAABetxEE/0cCAOJDAAAQIgAAQbAAAF0kAABCoA=='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(orig_X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = base64.b64decode(orig_X_train[0])\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_bytes = [x for x in decoded]\n",
    "print(int_bytes, len(int_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "hex_bytes = [ hex(x) for x in int_bytes]\n",
    "print(hex_bytes, len(hex_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want the values left 0-padded in a single string (doc)\n",
    "hex_bytes = binascii.hexlify(decoded).decode('utf-8')\n",
    "print(hex_bytes, len(hex_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# handy when developing etl in this notebook\n",
    "from importlib import reload\n",
    "if 'etl' in globals():\n",
    "    reload(etl)\n",
    "else:\n",
    "    import etl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hex_X_train = etl.hex_data(orig_X_train)\n",
    "hex_X_dev = etl.hex_data(orig_X_dev)\n",
    "hex_X_test = etl.hex_data(orig_X_test)\n",
    "\n",
    "print('hex_X_train[0:4]', hex_X_train[0:4], '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_vecs(X, num_indices):\n",
    "    \"\"\"\n",
    "    sample the representation of two vecs over first num_indices of vector\n",
    "    \"\"\"\n",
    "    for feature_0, freq_0, feature_1, freq_1 in zip(v.inverse_transform(X)[0][0:num_indices], \n",
    "                                                    X.A[0][0:num_indices],\n",
    "                                                    v.inverse_transform(X)[1][0:num_indices], \n",
    "                                                    X.A[1][0:num_indices],\n",
    "                                                   ):\n",
    "        print(feature_0, freq_0, \"   \", feature_1, freq_1)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_vecs(X_cv, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bow_transform2 = v.transform(hexdoc_X_train[0:2])\n",
    "print(bow_transform2)\n",
    "print(bow_transform2[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from IPython.core.debugger import set_trace\n",
    "\n",
    "vec_opts = {\n",
    "    \"ngram_range\": (1, 6),  # allow n-grams of 1-6 words in length (32-bits)\n",
    "    \"analyzer\": \"word\",     # analyze hex words\n",
    "    \"token_pattern\": \"..\",  # treat two characters as a word (e.g. 4b)\n",
    "    \"min_df\": 2,          # for demo purposes, be very selective about features\n",
    "    \"max_df\": .7\n",
    "}\n",
    "v = CountVectorizer(**vec_opts)\n",
    "X_cv = v.fit_transform(hex_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(v.vocabulary_.items())[0:10])\n",
    "print(len(v.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_cv.indices))\n",
    "print(type(X_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xd = X_cv.todense()\n",
    "print(Xd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xd.A[1][0:280])\n",
    "print(Xd.A[2][0:280])\n",
    "print(len(Xd.A[1]), len(Xd.A[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_cv.A[1][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "idf_opts = {\"use_idf\": True}\n",
    "idf = TfidfTransformer(**idf_opts)\n",
    "\n",
    "# perform the idf transform\n",
    "X_idf = idf.fit_transform(X_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnbClassifier = MultinomialNB(alpha=.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = mnbClassifier.fit(X_idf, np.array(orig_Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_class2 = MultinomialNB(alpha=.9)\n",
    "mnb_m2 = mnb_class2.fit(X_idf, np.array(orig_Y_train))\n",
    "mnb_m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_m2.coef_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = mnb_m2.coef_.argmax(axis=0) - mnb_model.coef_.argmax(axis=0)\n",
    "print(diff, len(diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(diff.nonzero(), len(diff.nonzero()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([[1,2,3], [3,4,5]]).argmax(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "### fraction of \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model.coef_ - mnb_m2.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = (mnb_model.coef_ - mnb_m2.coef_)*100.0 /mnb_model.coef_\n",
    "print(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions = mnb_model.predict(X_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(etl.SUPPORTED_ARCHITECTURES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnb_model.classes_.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Y_train, allowed_Y_train = class_to_ones_hot(orig_Y_train, orig_train_targets, mnb_model.classes_.tolist())\n",
    "Y_dev, allowed_Y_dev = class_to_ones_hot(orig_Y_dev, orig_dev_targets, mnb_model.classes_.tolist())\n",
    "Y_test, allowed_Y_test = class_to_ones_hot(orig_Y_test, orig_test_targets, mnb_model.classes_.tolist())\n",
    "\n",
    "print('Y_train[0:4]', '\\n', Y_train[0:4], '\\n')\n",
    "print('allowed_Y_train[0:4]', '\\n', allowed_Y_train[0:4], '\\n')\n",
    "print(list(map(\"\".join, hex_X_train[0:2])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_train = mnb_model.predict_proba(X_idf)\n",
    "print(\"raw probs\")\n",
    "print(probs_train[0:4])\n",
    "print(\"allowed probs\")\n",
    "print(probs_train[0:2]*allowed_Y_train[0:2])\n",
    "print(\"allowed\")\n",
    "print(allowed_Y_train[0:2])\n",
    "print(np.argmax(probs_train[0:2]*allowed_Y_train[0:2], axis=1))\n",
    "print(list(map(mnb_model.classes_.tolist().__getitem__, np.argmax(probs_train[0:10]*allowed_Y_train[0:10], axis=1))))\n",
    "print(\"correct\")\n",
    "print(orig_Y_train[0:10])\n",
    "print(Y_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_from_target(probs, allowed_Y, supported_architectures, ones_hot=True):\n",
    "    \"\"\"\n",
    "    Improve our chances by taking the max over the possible targets (6 instead of 12)\n",
    "    probs: numerical array of shape (m, n_classes)\n",
    "    targets: ones-hot array of shape (m, n_classes)\n",
    "    \n",
    "    returns: (m, 1) of the most likely ISA arch names after discards or\n",
    "             (m, n_classes) one-hot representation of best guess\n",
    "    \"\"\"\n",
    "    if ones_hot:\n",
    "        result = np.zeros(probs.shape)\n",
    "        result[np.argmax(probs*allowed_Y, axis=1)] = 1\n",
    "        return \n",
    "    return list(map(supported_architectures.__getitem__, np.argmax(probs*allowed_Y, axis=1)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(probs_train), len(allowed_Y_train))\n",
    "print(probs_train[0], allowed_Y_train[0], orig_train_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(probs_train*allowed_Y_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = guess_from_target(probs_train, allowed_Y_train, mnb_model.classes_.tolist(), ones_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0], orig_Y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = pd.DataFrame(predictions)\n",
    "D.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = pd.Series(orig_Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have a pretty good distribution of classes < 10% variance in counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_results(predictions, orig_Y):\n",
    "    wrong = []\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != orig_Y[i]:\n",
    "            wrong.append([predictions[i], orig_Y[i]])\n",
    "    print('training error: {}%'.format(100.0*len(wrong)/len(predictions)))\n",
    "    print('some mistakes')\n",
    "    print(wrong[0:15])\n",
    "    return wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = describe_results(predictions,orig_Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw = pd.DataFrame(wrong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw.columns = ['incorrect', 'correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw.groupby(by=pdw.correct).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model is worst at predicting sh4, then arm, m68k, xtensa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw.groupby(by=pdw.incorrect).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most false positives come from x86_64 and m68k, we are slightly over-fitting to them\n",
    "pdw[pdw.incorrect == 'm68k'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdw[pdw.correct == 'sparc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# repeat for dev dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vec',   CountVectorizer(**vec_opts)),\n",
    "    ('idf',  TfidfTransformer(**idf_opts))\n",
    "    #('mnb_classifier',MultinomialNB())\n",
    "])\n",
    "\n",
    "X_dev_idf = pipeline.fit_transform(hexdoc_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_cv_dev = v.transform(hex_X_dev, lowercase=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_opts = {\n",
    "    \"ngram_range\": (1, 4),  # allow n-grams of 1-4 words in length (32-bits)\n",
    "    \"analyzer\": \"word\",     # analyze hex words\n",
    "    \"token_pattern\": \"..\",  # treat two characters as a word (e.g. 4b)\n",
    "    \"min_df\": 3,          # \n",
    "    \"vocabulary\": v.vocabulary_\n",
    "}\n",
    "v_dev = CountVectorizer(**vec_opts)\n",
    "X_cv_dev = v_dev.fit_transform(hexdoc_X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf_dev = TfidfTransformer(**idf_opts)\n",
    "\n",
    "# perform the idf transform\n",
    "X_idf_dev = idf.fit_transform(X_cv_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idf_dev = idf.transform(X_cv_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_cv_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_idf_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs_dev = mnb_model.predict_proba(X_idf_dev)\n",
    "\n",
    "print(len(probs_dev), len(allowed_Y_dev))\n",
    "print(probs_dev[0], allowed_Y_dev[0], orig_dev_targets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_dev = guess_from_target(probs_dev, allowed_Y_dev, mnb_model.classes_.tolist(), ones_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong = describe_results(predictions_dev, orig_Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[7,2,4],[2,0,4]])\n",
    "b = np.array([[1,0,1], [0,0,1]])\n",
    "c = a*b\n",
    "print(c)\n",
    "d = np.zeros(c.shape)\n",
    "print(np.argmax(c, axis=1))\n",
    "print(d)\n",
    "d[np.argmax(c, axis=1)] = 1\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(c, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[ c >= np.max(c, axis=1, keepdims=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2], [2, 1]])\n",
    "b = np.array([[1,-1], [-1, 1]])\n",
    "print(a)\n",
    "print(b)\n",
    "print(a*b)\n",
    "print(np.multiply(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "\n",
    "def plot_hyperplane(clf, min_x, max_x, linestyle, label):\n",
    "    # get the separating hyperplane\n",
    "    w = clf.coef_[0]\n",
    "    a = -w[0] / w[1]\n",
    "    xx = np.linspace(min_x - 5, max_x + 5)  # make sure the line is long enough\n",
    "    yy = a * xx - (clf.intercept_[0]) / w[1]\n",
    "    plt.plot(xx, yy, linestyle, label=label)\n",
    "\n",
    "\n",
    "def plot_subfigure(X, Y, subplot, title, transform):\n",
    "    if transform == \"pca\":\n",
    "        X = PCA(n_components=12).fit_transform(X)\n",
    "    elif transform == \"cca\":\n",
    "        X = CCA(n_components=12).fit(X, Y_train).transform(X)\n",
    "    else:\n",
    "        raise ValueError\n",
    "\n",
    "    min_x = np.min(X[:, 0])\n",
    "    max_x = np.max(X[:, 0])\n",
    "\n",
    "    min_y = np.min(X[:, 1])\n",
    "    max_y = np.max(X[:, 1])\n",
    "\n",
    "    classif = SVC(kernel='linear')\n",
    "    classif.fit(X, Y_train.transpose())\n",
    "\n",
    "    plt.subplot(2, 2, subplot)\n",
    "    plt.title(title)\n",
    "\n",
    "    zero_class = np.where(Y_train[:, 0])\n",
    "    one_class = np.where(Y_train[:, 1])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=40, c='gray', edgecolors=(0, 0, 0))\n",
    "    plt.scatter(X[zero_class, 0], X[zero_class, 1], s=160, edgecolors='b',\n",
    "                facecolors='none', linewidths=2, label='Class 1')\n",
    "    plt.scatter(X[one_class, 0], X[one_class, 1], s=80, edgecolors='orange',\n",
    "                facecolors='none', linewidths=2, label='Class 2')\n",
    "\n",
    "    plot_hyperplane(classif.estimators_[0], min_x, max_x, 'k--',\n",
    "                    'Boundary\\nfor class 1')\n",
    "    plot_hyperplane(classif.estimators_[1], min_x, max_x, 'k-.',\n",
    "                    'Boundary\\nfor class 2')\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "\n",
    "    plt.xlim(min_x - .5 * max_x, max_x + .5 * max_x)\n",
    "    plt.ylim(min_y - .5 * max_y, max_y + .5 * max_y)\n",
    "    if subplot == 2:\n",
    "        plt.xlabel('First principal component')\n",
    "        plt.ylabel('Second principal component')\n",
    "        plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# X, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n",
    "#                                       allow_unlabeled=True,\n",
    "#                                       random_state=1)\n",
    "X_arr = X.toarray()\n",
    "\n",
    "plot_subfigure(X_arr, Y_train, 1, \"With unlabeled samples + CCA\", \"cca\")\n",
    "plot_subfigure(X_arr, Y, 2, \"With unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "# X, Y = make_multilabel_classification(n_classes=2, n_labels=1,\n",
    "#                                       allow_unlabeled=False,\n",
    "#                                       random_state=1)\n",
    "\n",
    "# plot_subfigure(X, Y, 3, \"Without unlabeled samples + CCA\", \"cca\")\n",
    "# plot_subfigure(X, Y, 4, \"Without unlabeled samples + PCA\", \"pca\")\n",
    "\n",
    "plt.subplots_adjust(.04, .02, .97, .94, .09, .2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xt, Yt = make_multilabel_classification(n_classes=12, n_labels=1,\n",
    "                                      allow_unlabeled=True,\n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yt[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Xt.shape, Yt.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print Y.shape\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}